{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecef6be6",
   "metadata": {},
   "source": [
    "# 本课内容\n",
    "\n",
    "1. 数据分析的重要性和应⽤场景\n",
    "2. Pandas数据读取、预处理和清洗\n",
    "    1. Pandas基础介绍\n",
    "    2. Pandas数据读取\n",
    "    3. Pandas数据预处理和清洗\n",
    "    4. Pandas数据分析\n",
    "3. 数据可视化的基本概念和工具\n",
    "    1. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1bba8",
   "metadata": {},
   "source": [
    "## 一、数据分析的重要性和应⽤场景\n",
    "\n",
    "数据分析在当今信息时代非常重要，它帮助组织和企业从海量数据中提取有价值的信息和见解，以支持决策制定、问题解决和业务优化。以下是数据分析的一些重要性和应用场景：\n",
    "\n",
    "1. 基于数据驱动的决策：数据分析帮助组织基于客观的数据来做出决策，而不是凭主观猜测或经验判断。通过分析数据，可以获取准确、可靠的信息，为决策提供依据。\n",
    "\n",
    "-  一个典型的基于数据驱动的决策的公司案例是亚马逊（Amazon）。亚马逊是全球最大的电子商务和云计算公司之一，其商业模式以数据驱动为核心。\n",
    "\n",
    "\n",
    "2. 发现趋势和模式：数据分析可以揭示数据中的趋势、模式和关联性。通过分析历史数据和当前数据，可以预测未来的趋势和模式，帮助组织做出准确的预测和规划。\n",
    "\n",
    "3. 优化业务流程：数据分析可以帮助组织识别业务流程中的瓶颈和问题，并提供改进的建议。通过分析数据，可以发现业务流程中的优化点，提高效率和效果。\n",
    "\n",
    "4. 客户洞察和市场分析：数据分析可以帮助组织了解客户的需求、行为和偏好，从而改善产品和服务，提供更好的客户体验。同时，数据分析也可以用于市场分析，了解市场趋势和竞争对手情况，制定市场策略。\n",
    "\n",
    "5. 风险管理和预测：数据分析可以帮助组织识别风险因素，并进行风险管理。通过分析数据，可以预测潜在的风险事件，采取相应的措施进行预防和应对。\n",
    "\n",
    "6. 战略规划和业务增长：数据分析可以为组织的战略规划和业务增长提供支持。通过分析市场和业务数据，可以识别新的机会和潜在的增长领域，制定相应的战略和计划。\n",
    "\n",
    "数据分析的应用场景非常广泛，涵盖了各个行业和领域。例如，在零售行业，数据分析可以用于销售预测、库存管理和客户行为分析；在金融行业，数据分析可以用于风险管理、信用评估和投资决策；在医疗健康领域，数据分析可以用于疾病预测、药物研发和患者管理等。\n",
    "\n",
    "总而言之，数据分析在当今的信息时代具有重要的价值和应用，可以帮助组织实现更好的决策、优化业务流程、提供更好的客户体验，并推动业务的增长和创新。\n",
    "\n",
    "### 业界案例\n",
    "\n",
    "\n",
    "**国内主流公司推荐系统架构**（其中很多地方的基础都是需要数据分析处理-构建算法等）\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "**架构层面包含数据排序层、融合过滤层、召回层、数据存储层、计算平台层、数据源等。这其中，不仅涉及多种算法逻辑，还关系到数据处理相关作业。**\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "1. 百度（Baidu）：作为中国最大的搜索引擎之一，百度利用大量的用户搜索数据来分析用户需求、行为和趋势。通过分析搜索关键词、点击率、搜索时间等数据，百度可以优化搜索算法，改进搜索结果的准确性和相关性。此外，百度还利用数据分析来优化广告投放策略，提高广告点击率和转化率。\n",
    "\n",
    "百度指数数据\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "2. 腾讯（Tencent）：作为中国最大的互联网公司之一，腾讯拥有丰富的用户数据资源，包括社交媒体、在线游戏、支付和电子商务等领域。腾讯通过分析用户行为数据，如社交互动、游戏活动、购买行为等，可以了解用户的兴趣、偏好和消费习惯。这些数据有助于腾讯优化产品功能、推荐个性化内容，以及制定精准的营销和广告策略。\n",
    "\n",
    "腾讯微信朋友圈广告推荐\n",
    "\n",
    "<!-- ![image-4.png](attachment:image-4.png) -->\n",
    "\n",
    "3. 阿里巴巴（Alibaba）：作为全球最大的电子商务公司之一，阿里巴巴利用大数据技术来分析和理解消费者行为和市场趋势。通过淘宝、天猫等平台的海量交易数据，阿里巴巴可以洞察消费者的购物偏好、价格敏感度和品牌偏好。这些数据被用于个性化推荐、精准营销、库存管理和供应链优化，以提供更好的购物体验和增加销售额。\n",
    "\n",
    "阿里巴巴淘宝首页推荐\n",
    "\n",
    "\n",
    "\n",
    "4. 京东（JD.com）：京东利用数据分析来提升用户购物体验和提供个性化服务。通过分析用户的浏览历史、购买行为和评价数据，京东可以为用户推荐相关产品、优化搜索结果、提供个性化的促销活动等。此外，京东还利用大数据分析来优化物流和仓储管理，以提高配送效率和降低运营成本。\n",
    "\n",
    "\n",
    "<!-- 5. 小红书笔记数据分析\n",
    "![image-5.png](attachment:image-5.png)\n",
    "![image-6.png](attachment:image-6.png)\n",
    "![image-7.png](attachment:image-7.png)\n",
    "![image-8.png](attachment:image-8.png) -->\n",
    "\n",
    "这些企业通过收集、处理和分析大量的数据，从中获取有价值的信息和洞察力，以支持业务决策和提升用户体验。数据驱动的决策帮助它们更好地了解用户需求、优化产品和服务，并制定精准的营销策略，从而实现业务增长和竞争优势。\n",
    "\n",
    "\n",
    "### 数据的生命周期\n",
    "\n",
    "数据的生命周期是指数据从创建、存储、使用、维护到最终删除的整个过程。它描述了数据在不同阶段的处理和管理方式，以确保数据的完整性、安全性和有效性。\n",
    "\n",
    "通常，数据的生命周期可以划分为以下几个阶段：\n",
    "\n",
    "1. 数据采集：在这个阶段，数据被收集、记录和捕获。它可以来自各种来源，如传感器、数据库、文件、Web等。数据采集的目的是获取所需的数据以进行后续处理和分析。\n",
    "\n",
    "2. 数据存储：采集到的数据需要被存储起来以便后续使用。数据存储可以包括数据库、文件系统、云存储等方式。在存储阶段，需要考虑数据的安全性、可靠性和可扩展性。\n",
    "\n",
    "3. 数据处理和分析：在这个阶段，对存储的数据进行处理和分析，以提取有用的信息和洞察力。数据处理可以包括清洗、转换、整合、聚合等操作，而数据分析则涉及统计分析、机器学习、数据挖掘等技术。\n",
    "\n",
    "4. 数据应用：经过处理和分析后的数据可以被应用于不同的领域和场景。例如，数据可以用于制定决策、支持业务运营、改进产品设计、优化营销策略等。数据应用的目的是将数据转化为有意义和有价值的信息。\n",
    "\n",
    "5. 数据维护和管理：数据需要得到持续的维护和管理，以确保数据的准确性、完整性和可用性。这包括数据的备份、恢复、版本控制、权限管理、数据质量监控等活动。\n",
    "\n",
    "6. 数据删除和归档：在一定的时间段后，一些数据可能不再具有业务或分析的价值。在这种情况下，数据可以被删除或归档。数据删除和归档需要遵循相关的法规和规定，并确保数据的安全和隐私。\n",
    "\n",
    "数据的生命周期管理有助于组织和管理数据资产，使其在不同阶段得到有效的处理和利用。通过合理规划和实施数据的生命周期，可以最大限度地发挥数据的价值，支持业务决策和创新。\n",
    "\n",
    "\n",
    "本课我们主要学习数据处理和分析这部分。包括以下步骤：\n",
    "\n",
    "- 数据读取\n",
    "- 数据处理\n",
    "- 数据清洗\n",
    "- 数据分析\n",
    "\n",
    "而我们主要使用的工具就是Pandas库。\n",
    "\n",
    "## 二、Pandas库\n",
    "\n",
    "参考材料：\n",
    "\n",
    "- [joyful-pandas学习笔记](https://github.com/yeayee/joyful-pandas)\n",
    "- [Pandas菜鸟教程](https://www.runoob.com/pandas/pandas-tutorial.html)\n",
    "\n",
    "### 2.1 简介\n",
    "\n",
    "Pandas是一个强大、灵活、易于使用的开源Python库，用于数据分析和数据处理。它为Python编程语言提供了高级数据结构，特别是DataFrame，这是一个表格型的数据结构，类似于SQL中的表或Excel中的电子表格。Pandas的设计目标是使数据操作变得简单、直观和高效。\n",
    "\n",
    "以下是Pandas库的主要特点和功能：\n",
    "\n",
    "1. DataFrame：Pandas的核心数据结构是DataFrame。它可以将不同类型的数据（如数字、字符串、布尔值等）组织成二维表格，类似于Excel中的电子表格。DataFrame具有强大的数据索引和标签功能，可以方便地进行数据切片、过滤和处理。\n",
    "\n",
    "2. 数据清洗：Pandas提供了丰富的功能来处理缺失值、重复数据和异常值。通过内置的方法，可以轻松地对数据进行清洗和转换，以便后续的数据分析和建模。\n",
    "\n",
    "3. 数据读取与存储：Pandas支持从多种数据源中读取数据，如CSV、Excel、数据库、JSON等。同时，它还可以将数据导出为不同格式的文件。\n",
    "\n",
    "4. 数据分组和聚合：Pandas提供了灵活的分组功能，可以根据某些标准将数据进行分组，然后对每个组进行聚合操作（如求和、平均值、计数等）。\n",
    "\n",
    "5. 时间序列处理：Pandas对时间序列数据有着良好的支持。它可以处理时间和日期数据，并提供了一系列时间频率转换和重采样的工具。\n",
    "\n",
    "6. 数据合并和连接：Pandas允许将多个数据集合并和连接成一个，类似于SQL中的JOIN操作。\n",
    "\n",
    "7. 强大的性能：Pandas基于NumPy构建，并且使用了很多NumPy的功能，因此在处理大规模数据时，它能够提供高效的性能。\n",
    "\n",
    "Pandas是数据科学和数据分析领域的重要工具，因为它使数据处理和分析变得更加简单和高效。它与其他Python库（如NumPy、Matplotlib和Scikit-learn等）配合使用，为数据科学家和分析师提供了一个强大而全面的工具生态系统。\n",
    "\n",
    "\n",
    "### 2.2 Pandas库主要类\n",
    "\n",
    "1. `DataFrame（数据帧）`：DataFrame是Pandas中最常用的数据结构，它类似于一个二维表格，可以存储不同类型的数据。DataFrame可以通过多种方式创建，比如从列表、字典、CSV文件等。\n",
    "\n",
    "2. `Series（序列）`：Series是Pandas中另一个常用的数据结构，它类似于一个一维数组，可以存储同类型的数据。Series可以看作是DataFrame的一列。\n",
    "\n",
    "3. `Index（索引）`：Index是DataFrame和Series的标签，它用来对数据进行标识和检索。在DataFrame中，每一行和每一列都有一个唯一的Index标签。\n",
    "\n",
    "4. `Panel（面板）`：Panel是Pandas中用于存储三维数据的数据结构，不过在最新的Pandas版本中已经被废弃，推荐使用MultiIndex和DataFrame来代替。\n",
    "\n",
    "这些类是Pandas中最核心、最重要的数据结构，它们提供了丰富的方法和属性，可以方便地对数据进行操作、处理和分析。通过灵活运用这些类，可以高效地处理大规模的数据，并进行数据清洗、转换、聚合等操作。\n",
    "\n",
    "请注意，Pandas中还有许多其他类和数据结构，如MultiIndex、Categorical等，它们也在一些特定场景下起着重要的作用。不过对于大部分数据分析任务而言，DataFrame和Series是最为常用和重要的两个类。\n",
    "\n",
    "#### 2.2.1 Series\n",
    "\n",
    "在`Pandas`中，`Series`是一个重要的数据结构，它是一维带标签的数组。可以将它看作是一个类似于Python列表的对象，但与列表不同，`Series`具有额外的功能和灵活性。\n",
    "\n",
    "`Series`由两个主要部分组成：索引（`index`）和值（`values`）。索引是一组标签，用于标识`Series`中的每个元素，而值是与每个索引相关联的数据。\n",
    "\n",
    "创建一个`Series`可以通过多种方式，比如从Python列表、`NumPy`数组或字典等数据结构。以下是一些创建`Series`的例子：\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 从列表创建Series\n",
    "data_list = [10, 20, 30, 40, 50]\n",
    "series_from_list = pd.Series(data_list)\n",
    "\n",
    "# 从NumPy数组创建Series\n",
    "import numpy as np\n",
    "data_array = np.array([1.1, 2.2, 3.3, 4.4, 5.5])\n",
    "series_from_array = pd.Series(data_array)\n",
    "\n",
    "# 从字典创建Series\n",
    "data_dict = {'a': 100, 'b': 200, 'c': 300}\n",
    "series_from_dict = pd.Series(data_dict)\n",
    "```\n",
    "\n",
    "Series的特点包括：\n",
    "\n",
    "##### 索引标签\n",
    "\n",
    "在Pandas的Series中，索引标签（index label）是用来标识每个元素的唯一标签或名称。它们是Series的一部分，与每个元素的值相关联，类似于Python字典中的键。索引标签允许我们通过标签而不是位置来引用和操作Series中的数据，这为数据访问和处理带来了更大的灵活性和可读性。\n",
    "\n",
    "默认情况下，当创建一个Series时，如果没有显式指定索引，Pandas会自动创建一个从0开始的整数索引作为标签。例如：\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = [10, 20, 30, 40, 50]\n",
    "series = pd.Series(data)\n",
    "```\n",
    "\n",
    "在这个例子中，series对象的索引标签将是`0, 1, 2, 3, 4`，与数据列表中的元素一一对应。\n",
    "\n",
    "但是，您也可以通过传递自定义的索引列表来创建Series，并使用这些标签来标识数据。例如：\n",
    "\n",
    "```python\n",
    "data = [10, 20, 30, 40, 50]\n",
    "custom_index = ['a', 'b', 'c', 'd', 'e']\n",
    "series = pd.Series(data, index=custom_index)\n",
    "```\n",
    "\n",
    "现在，series对象的索引标签将是`'a', 'b', 'c', 'd', 'e'`，而不是默认的整数索引。\n",
    "\n",
    "索引标签的重要性在于它们使得我们可以通过标签来选择、筛选和操作Series中的数据。例如：\n",
    "\n",
    "```python\n",
    "print(series['b'])  # 输出 20\n",
    "print(series[['c', 'd']])  # 输出 c 和 d 对应的值\n",
    "```\n",
    "\n",
    "另外，当进行多个Series之间的运算时，Pandas会根据索引标签自动对齐数据，这是一个非常有用的特性。例如，如果有两个Series对象，它们有不同的索引标签，当对它们进行加法运算时，Pandas会根据标签对齐数据并生成新的Series。\n",
    "\n",
    "总结一下，Series索引标签是Pandas中重要的组成部分，它们提供了更具描述性和可读性的数据访问方式，并在数据操作中发挥着重要的作用。\n",
    "\n",
    "\n",
    "##### 矢量化操作\n",
    "\n",
    "在Pandas中，Series支持矢量化操作，这意味着您可以对整个Series进行数学运算或其他操作，而不需要编写循环。这样的矢量化操作使得数据处理更加高效和简洁。\n",
    "\n",
    "矢量化操作是通过广播（broadcasting）机制实现的，它使得Series与标量、其他Series对象或NumPy数组之间的操作成为可能。Pandas会根据元素的索引标签自动对齐数据，因此您无需担心索引的顺序或缺失值的问题。\n",
    "\n",
    "以下是一些常见的Series矢量化操作示例：\n",
    "\n",
    "1. 算术运算：对Series中的所有元素进行数学运算，例如加法、减法、乘法和除法。\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = [10, 20, 30, 40, 50]\n",
    "series = pd.Series(data)\n",
    "\n",
    "# 加法\n",
    "result = series + 5\n",
    "# 或者\n",
    "result = series.add(5)\n",
    "\n",
    "# 减法\n",
    "result = series - 10\n",
    "# 或者\n",
    "result = series.sub(10)\n",
    "\n",
    "# 乘法\n",
    "result = series * 2\n",
    "# 或者\n",
    "result = series.mul(2)\n",
    "\n",
    "# 除法\n",
    "result = series / 3\n",
    "# 或者\n",
    "result = series.div(3)\n",
    "```\n",
    "\n",
    "2. 数学函数：应用各种数学函数到Series的每个元素。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 平方根\n",
    "result = np.sqrt(series)\n",
    "\n",
    "# 指数函数\n",
    "result = np.exp(series)\n",
    "\n",
    "# 对数函数\n",
    "result = np.log(series)\n",
    "```\n",
    "\n",
    "3. 逻辑运算：应用逻辑运算符对Series中的元素进行逐元素比较。\n",
    "\n",
    "```python\n",
    "# 大于等于操作\n",
    "result = series >= 30\n",
    "\n",
    "# 逻辑与操作\n",
    "result = (series > 20) & (series < 50)\n",
    "\n",
    "# 逻辑或操作\n",
    "result = (series == 10) | (series == 40)\n",
    "```\n",
    "\n",
    "4. 统计函数：应用统计函数对Series的所有元素进行汇总计算。\n",
    "\n",
    "```python\n",
    "# 求和\n",
    "result = series.sum()\n",
    "\n",
    "# 平均值\n",
    "result = series.mean()\n",
    "\n",
    "# 标准差\n",
    "result = series.std()\n",
    "\n",
    "# 最大值\n",
    "result = series.max()\n",
    "\n",
    "# 最小值\n",
    "result = series.min()\n",
    "```\n",
    "\n",
    "math_grade = Series([90, 95, 60, 70, 59])\n",
    "\n",
    "normalize_grade = (math_grade - math_grade.mean()) / math_grade.std()\n",
    "df['n_math_grade'] = normalize_grade\n",
    "\n",
    "通过矢量化操作，您可以简洁地处理Series中的数据，而不必逐个元素进行循环和操作。这不仅使代码更加优雅，而且在处理大规模数据时提供了显著的性能优势。\n",
    "\n",
    "##### 缺失值处理\n",
    "\n",
    "在Pandas中，处理缺失值是数据清洗的一个重要步骤。缺失值是指数据集中的某些条目或字段没有具体的值，通常表示为NaN（Not a Number）。Pandas提供了一些方法来处理Series中的缺失值，让您能够根据需要选择适当的处理方式。\n",
    "\n",
    "以下是常见的Series缺失值处理方法：\n",
    "\n",
    "1. 检测缺失值：\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = [10, None, 30, None, 50]\n",
    "series = pd.Series(data)\n",
    "\n",
    "# 检测缺失值\n",
    "print(series.isnull())\n",
    "\n",
    "# 检测非缺失值\n",
    "print(series.notnull())\n",
    "```\n",
    "\n",
    "2. 删除缺失值：\n",
    "\n",
    "```python\n",
    "# 删除包含缺失值的行\n",
    "cleaned_series = series.dropna()\n",
    "```\n",
    "\n",
    "3. 填充缺失值：\n",
    "\n",
    "`fillna`函数的原型如下：\n",
    "\n",
    "`Series.fillna()`是Pandas中的一个方法，用于填充Series中的缺失值。缺失值在Pandas中通常表示为NaN（Not a Number）。`fillna()`方法允许您根据指定的规则或值来填充缺失值，以便更好地处理数据。下面是`Series.fillna()`方法的详细说明：\n",
    "\n",
    "**语法：**\n",
    "```python\n",
    "Series.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)\n",
    "```\n",
    "\n",
    "**参数说明：**\n",
    "- `value`: 可选参数，用于指定用来填充缺失值的值。它可以是一个具体的数值、字典、Series或DataFrame。默认为None。\n",
    "- `method`: 可选参数，用于指定填充缺失值的方法。它可以取以下几个值：\n",
    "  - None（默认）：不使用任何方法进行填充。\n",
    "  - 'ffill'或'pad'：使用前一个非缺失值进行前向填充。\n",
    "  - 'bfill'或'backfill'：使用后一个非缺失值进行后向填充。\n",
    "- `axis`: 可选参数，用于指定填充缺失值的轴。它可以是0（按行填充）或1（按列填充）。默认为None，表示按照Series的索引标签填充。\n",
    "- `inplace`: 可选参数，如果为True，则会就地修改原始Series对象，而不返回新的Series对象。默认为False。\n",
    "- `limit`: 可选参数，用于指定连续缺失值的最大填充次数。默认为None，表示没有限制。\n",
    "- `downcast`: 可选参数，用于控制数据类型的转换。它可以取值'integer'、'signed'、'unsigned'或False。默认为None，表示不进行数据类型转换。\n",
    "\n",
    "**返回值：**\n",
    "- 如果`inplace=True`，则返回值为None，表示原始Series对象被就地修改。\n",
    "- 如果`inplace=False`，则返回填充了缺失值的新Series对象。\n",
    "\n",
    "\n",
    "**示例**\n",
    "```python\n",
    "# 使用特定值填充缺失值\n",
    "filled_series = series.fillna(0)  # 使用0填充缺失值\n",
    "\n",
    "# 使用前一个有效值填充缺失值（前向填充）\n",
    "filled_series = series.fillna(method='ffill')\n",
    "\n",
    "# 使用后一个有效值填充缺失值（后向填充）\n",
    "filled_series = series.fillna(method='bfill')\n",
    "```\n",
    "\n",
    "\n",
    "4. 插值填充缺失值：\n",
    "\n",
    "```python\n",
    "# 使用线性插值填充缺失值\n",
    "filled_series = series.interpolate()\n",
    "\n",
    "# 使用更复杂的插值方法，如二次插值\n",
    "filled_series = series.interpolate(method='quadratic')\n",
    "```\n",
    "\n",
    "5. 删除全部是缺失值的行：\n",
    "\n",
    "```python\n",
    "# 删除全部是缺失值的行\n",
    "cleaned_series = series.dropna(how='all')\n",
    "```\n",
    "\n",
    "这些方法可以根据数据的实际情况来选择使用。在处理缺失值时，需要根据具体业务场景和数据分布选择合适的方法。有时候，删除缺失值可能导致数据丢失过多，而填充缺失值可能引入不准确的数据。因此，需要谨慎处理缺失值，以确保数据的准确性和可靠性。\n",
    "\n",
    "请注意，以上方法都返回一个新的Series对象，并且不会修改原始的Series对象。如果要对原始Series进行就地修改，可以使用`inplace=True`参数，例如：`series.dropna(inplace=True)`或`series.fillna(0, inplace=True)`。\n",
    "\n",
    "\n",
    "##### 对齐功能\n",
    "\n",
    "在Pandas中，Series具有强大的对齐功能，它允许在进行运算或操作时，根据索引标签自动对齐不同的Series对象。这种对齐功能在处理多个Series对象时非常有用，它确保数据按照正确的索引对齐，避免了由于索引标签不匹配而导致的错误或数据错位。\n",
    "\n",
    "假设有两个Series对象，它们具有不同的索引标签：\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data1 = [10, 20, 30]\n",
    "index1 = ['a', 'b', 'c']\n",
    "series1 = pd.Series(data1, index=index1)\n",
    "\n",
    "data2 = [100, 200, 300]\n",
    "index2 = ['b', 'c', 'd']\n",
    "series2 = pd.Series(data2, index=index2)\n",
    "```\n",
    "\n",
    "Series1：\n",
    "```\n",
    "a    10\n",
    "b    20\n",
    "c    30\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "Series2：\n",
    "```\n",
    "b    100\n",
    "c    200\n",
    "d    300\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "在进行运算时，Pandas会根据索引标签自动对齐数据，如果索引标签匹配，将执行运算，否则对应位置的元素将被视为缺失值（NaN）：\n",
    "\n",
    "```python\n",
    "# 对两个Series对象相加\n",
    "result = series1 + series2\n",
    "```\n",
    "\n",
    "结果将会是：\n",
    "```\n",
    "a      NaN  # 因为索引标签 'a' 在 series2 中不存在，所以为缺失值\n",
    "b    120.0  # 对应位置的元素 20 + 100\n",
    "c    230.0  # 对应位置的元素 30 + 200\n",
    "d      NaN  # 因为索引标签 'd' 在 series1 中不存在，所以为缺失值\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "正因为对齐功能，我们不需要担心两个Series之间的索引顺序或缺失值的问题。Pandas会自动处理这些情况，确保计算结果的准确性和完整性。\n",
    "\n",
    "对齐功能在进行数据合并、运算、聚合等操作时都非常有用，它使得处理复杂的数据集变得更加简单和可靠。\n",
    "\n",
    "\n",
    "\n",
    "Series是Pandas中许多其他功能的基础，例如DataFrame中的列就是一个Series对象。它在数据处理和分析中广泛使用，为用户提供了一种更加便捷和高效的数据存储和处理方式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d821c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m50\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 检测缺失值\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(series\u001b[38;5;241m.\u001b[39misnull())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Series'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [10, None, 30, None, 50]\n",
    "series = pd.Series(data)\n",
    "\n",
    "# 检测缺失值\n",
    "print(series.isnull())\n",
    "\n",
    "# 检测非缺失值\n",
    "print(series.notnull())\n",
    "\n",
    "\n",
    "cleaned_series = series.dropna()\n",
    "\n",
    "print(cleaned_series)\n",
    "\n",
    "\n",
    "fs = series.fillna(method='ffill')\n",
    "print(fs)\n",
    "\n",
    "bs = series.fillna(method='bfill')\n",
    "print(bs)\n",
    "\n",
    "series.fillna(method='bfill', inplace=True)\n",
    "series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e789d2",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.2.2 DataFrame\n",
    "\n",
    "DataFrame是pandas库中最常用的数据结构之一，用于处理二维表格数据。\n",
    "\n",
    "Pandas提供了多种读取数据的函数，它们允许您从各种数据源中读取数据，并将其转换为DataFrame或Series对象。以下是一些常用的Pandas读取函数：\n",
    "\n",
    "1. 从CSV文件读取：\n",
    "   - `pd.read_csv(filepath_or_buffer)`: 从CSV文件中读取数据，并返回一个DataFrame对象。分隔符是(',')\n",
    "\n",
    "2. 从Excel文件读取：\n",
    "    - f = open('test.xslx', \"r\")\n",
    "   - `pd.read_excel(io, sheet_name)`: 从Excel文件中读取数据，并返回一个DataFrame对象。\n",
    "   - `pd.read_excel(io, sheet_name=None)`: 从Excel文件中读取所有工作表数据，并返回一个字典，键为工作表名称，值为对应的DataFrame对象。\n",
    "\n",
    "3. 从文本文件读取：\n",
    "   - `pd.read_table(filepath_or_buffer)`: 从文本文件中读取数据，并返回一个DataFrame对象。默认分隔符为制表符（'\\t'）。\n",
    "\n",
    "4. 从SQL数据库读取：\n",
    "   - `pd.read_sql(query, connection_object)`: 从SQL数据库中读取数据，并返回一个DataFrame对象。\n",
    "\n",
    "5. 从剪贴板读取：\n",
    "   - `pd.read_clipboard()`: 从剪贴板中读取数据，并返回一个DataFrame对象。\n",
    "\n",
    "6. 从HTML文件读取：\n",
    "   - `pd.read_html(url)`: 从HTML文件中读取所有表格数据，并返回一个包含DataFrame对象的列表。\n",
    "\n",
    "7. 从JSON文件读取：\n",
    "   - `pd.read_json(path_or_buf)`: 从JSON文件中读取数据，并返回一个DataFrame对象。\n",
    "\n",
    "8. 从字典或其他数据结构读取：\n",
    "   - `pd.DataFrame(data)`: 从字典、列表、NumPy数组等数据结构创建DataFrame对象。\n",
    "\n",
    "以上是一些常见的Pandas读取函数，它们允许您从多种数据源中读取数据，并将其转换为Pandas的DataFrame或Series对象，方便后续的数据处理和分析。具体使用哪个函数取决于您的数据源类型以及数据的格式。请注意，这些函数的参数可能有所不同，具体的参数配置请查阅Pandas官方文档。\n",
    "\n",
    "那我们常用的其实是从python数据结构、csv、json读取数据创建DataFrame进行分析处理。\n",
    "\n",
    "\n",
    "下面列举一些DataFrame的常用方法：\n",
    "\n",
    "1. 查看数据：\n",
    "\n",
    "- `head(n)`: 显示DataFrame的前n行，默认为5行。\n",
    "- `tail(n)`: 显示DataFrame的后n行，默认为5行。\n",
    "- `shape`: 返回DataFrame的行数和列数。\n",
    "- `info()`: 显示DataFrame的基本信息，包括列名、数据类型、非空值数量等。\n",
    "- `describe()`: 统计DataFrame中每列的基本统计信息，如均值、标准差等。\n",
    "\n",
    "2. 数据选择：\n",
    "\n",
    "- `loc[]`: 通过标签索引选择行或列。例如：\n",
    "    - `df.loc[0]`选择第一行\n",
    "    - `df.loc[:, 'Name']` 选择 索引为Name的列\n",
    "    - `df.loc[0, 'Name']` 选择行索引为0，列索引为Name的值\n",
    "- `iloc[]`: 通过整数索引选择行或列。\n",
    "    - `df.iloc[0]`第一行\n",
    "    - `df.iloc[:, 0]`第一列\n",
    "    - `df.iloc[0, 0]`第一行第一列的元素\n",
    "- `at[]`: 通过标签精确选择单个元素。\n",
    "- `iat[]`: 通过整数索引精确选择单个元素。\n",
    "- `[]`: 通过标签或整数索引选择行或列。\n",
    "\n",
    "\n",
    "注意：\n",
    "\n",
    "`at[]`和`loc[]`都是Pandas中用于访问DataFrame或Series中的数据的方法，但它们之间有一些重要的区别：\n",
    "\n",
    "1. 用途和语法：\n",
    "\n",
    "   - `at[]`：用于在DataFrame或Series中使用标签精确定位单个元素的值。它的语法是`df.at[row_label, col_label]`，其中`row_label`是行标签，`col_label`是列标签。\n",
    "\n",
    "   - `loc[]`：用于通过标签选择DataFrame或Series中的一行或多行数据，或者选择一行中的某个元素（类似于`at[]`）。它的语法是`df.loc[row_label, col_label]`，其中`row_label`是行标签，`col_label`是列标签。不过需要注意，`loc[]`也可以通过切片、布尔索引等方式选择多行或多个元素。\n",
    "\n",
    "2. 使用场景：\n",
    "\n",
    "   - `at[]`：适用于快速精确定位某个特定元素，特别是当DataFrame或Series具有大量数据时，使用`at[]`可以更高效地获取特定位置的元素。\n",
    "\n",
    "   - `loc[]`：适用于通过标签进行数据的选择和过滤。`loc[]`支持通过标签选择一行或多行，以及选择一行中的某个元素。同时，`loc[]`也支持切片、布尔索引等更复杂的数据选择操作。\n",
    "\n",
    "3. 效率：\n",
    "\n",
    "   - `at[]`：由于其目的是精确定位单个元素，所以相对于`loc[]`而言，`at[]`更高效，特别是在DataFrame中具有大量数据时。\n",
    "\n",
    "   - `loc[]`：虽然`loc[]`更通用，但由于它支持更复杂的数据选择操作，所以在处理大规模数据时可能比`at[]`稍慢。但是，Pandas在底层对`loc[]`进行了优化，因此对于大部分数据集而言，性能差异并不明显。\n",
    "\n",
    "综上所述，`at[]`适用于精确定位单个元素，而`loc[]`适用于更复杂的数据选择和过滤操作。在选择单个元素时，如果您知道元素的具体位置，则可以使用`at[]`来获取更高的性能。而在进行更复杂的数据选择时，使用`loc[]`是更为灵活和通用的选择。\n",
    "\n",
    "\n",
    "3. 数据操作：\n",
    "\n",
    "- `drop()`: 删除指定行或列。\n",
    "\n",
    "`drop()`是Pandas中用于删除DataFrame或Series中指定行或列的方法。它允许您根据行标签或列标签来删除数据，或者在删除时指定要删除的轴。以下是`drop()`方法的详细说明：\n",
    "\n",
    "**语法：**\n",
    "\n",
    "对于DataFrame对象：\n",
    "```python\n",
    "DataFrame.drop(labels, axis=0, inplace=False)\n",
    "```\n",
    "\n",
    "对于Series对象：\n",
    "```python\n",
    "Series.drop(labels, inplace=False)\n",
    "```\n",
    "\n",
    "**参数说明：**\n",
    "\n",
    "- `labels`：必需参数，用于指定要删除的行标签或列标签。可以是单个标签、标签列表或标签切片。\n",
    "\n",
    "- `axis`：可选参数，用于指定删除的轴。可以是0（按行删除）或1（按列删除）。默认为0，即按行删除。\n",
    "\n",
    "- `inplace`：可选参数，如果为True，则会就地修改原始DataFrame或Series对象，而不返回新的对象。默认为False。\n",
    "\n",
    "**返回值：**\n",
    "\n",
    "对于DataFrame，如果`inplace=True`，则返回值为None，表示原始DataFrame对象被就地修改；如果`inplace=False`，则返回一个新的DataFrame对象，不改变原始对象。\n",
    "\n",
    "对于Series，如果`inplace=True`，则返回值为None，表示原始Series对象被就地修改；如果`inplace=False`，则返回一个新的Series对象，不改变原始对象。\n",
    "\n",
    "**示例：**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [25, 30, 35],\n",
    "        'City': ['New York', 'London', 'Paris']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 删除指定行\n",
    "df_drop_row = df.drop(1)  # 删除索引为1的行\n",
    "\n",
    "# 删除指定列\n",
    "df_drop_column = df.drop('Age', axis=1)  # 删除'Age'列\n",
    "\n",
    "# 就地修改原始DataFrame，删除指定行\n",
    "df.drop(2, inplace=True)  # 删除索引为2的行\n",
    "```\n",
    "\n",
    "在上面的示例中，我们演示了`drop()`方法在DataFrame中的使用。通过指定要删除的行标签或列标签，您可以删除DataFrame中的指定行或列。请注意，在默认情况下，`drop()`方法返回一个新的DataFrame对象，不会修改原始对象。如果希望就地修改原始对象，可以将`inplace=True`参数传递给`drop()`方法。\n",
    "\n",
    "\n",
    "- `copy()`: 复制DataFrame。\n",
    "- `append()`: 将另一个DataFrame或Series添加到当前DataFrame。\n",
    "- `fillna()`: 填充缺失值。\n",
    "- `replace()`: 替换特定值。\n",
    "\n",
    "`replace()`是Pandas中用于替换DataFrame或Series中的值的方法。它允许您根据指定的规则，将DataFrame或Series中的某些值替换为新的值。`replace()`方法可以处理特定的值、多个值或者使用字典进行替换。以下是`replace()`方法的详细说明：\n",
    "\n",
    "**语法：**\n",
    "\n",
    "对于DataFrame对象：\n",
    "```python\n",
    "DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')\n",
    "```\n",
    "\n",
    "对于Series对象：\n",
    "```python\n",
    "Series.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')\n",
    "```\n",
    "\n",
    "**参数说明：**\n",
    "\n",
    "- `to_replace`：必需参数，用于指定要替换的值。可以是单个值、多个值组成的列表、字典或者用于匹配的正则表达式。\n",
    "\n",
    "- `value`：必需参数，用于指定要替换为的新值。可以是单个值或者多个值组成的列表。如果`to_replace`是字典或正则表达式，则`value`应为None。\n",
    "\n",
    "- `inplace`：可选参数，如果为True，则会就地修改原始DataFrame或Series对象，而不返回新的对象。默认为False。\n",
    "\n",
    "- `limit`：可选参数，用于限制替换的次数。默认为None，表示没有限制。\n",
    "\n",
    "- `regex`：可选参数，如果为True，则将`to_replace`和`value`视为正则表达式，并按照正则表达式进行匹配和替换。默认为False。\n",
    "\n",
    "- `method`：可选参数，用于指定替换时使用的方法。它可以取以下几个值：\n",
    "  - 'pad'或'ffill'：向前填充，使用前一个非缺失值进行替换。\n",
    "  - 'bfill'或'backfill'：向后填充，使用后一个非缺失值进行替换。\n",
    "\n",
    "**返回值：**\n",
    "\n",
    "对于DataFrame和Series，如果`inplace=True`，则返回值为None，表示原始对象被就地修改；如果`inplace=False`，则返回一个新的DataFrame或Series对象，不改变原始对象。\n",
    "\n",
    "**示例：**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 23, 40],\n",
    "        'City': ['New York 23', 'London 23', 'Paris 23', 'Sydney 23']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 将年龄中的值 30 替换为 32\n",
    "df_replace_single_value = df.replace(to_replace=30, value=32)\n",
    "\n",
    "# 将名字中的 Bob 替换为 Robert，将年龄中的 30 替换为 32\n",
    "df_replace_multiple_values = df.replace(to_replace={'Name': 'Bob', 'Age': 30}, value={'Name': 'Robert', 'Age': 32})\n",
    "\n",
    "# 使用正则表达式将 City 中的所有 'Y' 替换为 'YY'\n",
    "df_replace_regex = df.replace(to_replace=r'\\d+', value='100', regex=True)\n",
    "\n",
    "# 就地修改原始DataFrame，将年龄中的值 30 替换为 32\n",
    "df.replace(to_replace=30, value=32, inplace=True)\n",
    "```\n",
    "\n",
    "以上示例中，我们演示了`replace()`方法在DataFrame中的使用。通过指定`to_replace`和`value`参数，您可以根据指定的规则将DataFrame或Series中的值替换为新的值。可以根据实际数据需求，选择合适的替换规则和方法。\n",
    "\n",
    "\n",
    "- `groupby()`: 对数据进行分组操作。\n",
    "\n",
    "4. 数据排序：\n",
    "\n",
    "- `sort_values()`: 按指定列的值对DataFrame进行排序。\n",
    "\n",
    "`sort_values()`是Pandas中用于对DataFrame或Series按照指定的列或索引进行排序的方法。它允许您根据某一列或多列的值，对DataFrame中的行进行排序，或者对Series中的元素进行排序。`sort_values()`方法可以实现升序排序和降序排序。以下是`sort_values()`方法的详细说明：\n",
    "\n",
    "**语法：**\n",
    "\n",
    "对于DataFrame对象：\n",
    "```python\n",
    "DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, ignore_index=False, na_position='last')\n",
    "```\n",
    "\n",
    "对于Series对象：\n",
    "```python\n",
    "Series.sort_values(axis=0, ascending=True, inplace=False, ignore_index=False)\n",
    "```\n",
    "\n",
    "**参数说明：**\n",
    "\n",
    "- `by`：必需参数，用于指定按照哪一列或多列的值进行排序。可以是单个列名或列名的列表。\n",
    "\n",
    "- `axis`：可选参数，用于指定排序的轴。对于DataFrame，可以是0（按行排序）或1（按列排序）。对于Series，只能是0。默认为0。\n",
    "\n",
    "- `ascending`：可选参数，用于指定排序顺序。如果为True，则表示升序排序；如果为False，则表示降序排序。默认为True。\n",
    "\n",
    "- `inplace`：可选参数，如果为True，则会就地修改原始DataFrame或Series对象，而不返回新的对象。默认为False。\n",
    "\n",
    "- `ignore_index`：可选参数，如果为True，则在排序后重置行索引（DataFrame）或标签（Series）为默认整数索引。默认为False。\n",
    "\n",
    "- `na_position`：可选参数，用于指定缺失值的位置。它可以取以下几个值：\n",
    "  - 'first'：将缺失值放在排序结果的最前面。\n",
    "  - 'last'：将缺失值放在排序结果的最后面。默认值。\n",
    "\n",
    "**返回值：**\n",
    "\n",
    "对于DataFrame和Series，如果`inplace=True`，则返回值为None，表示原始对象被就地修改；如果`inplace=False`，则返回一个新的DataFrame或Series对象，不改变原始对象。\n",
    "\n",
    "**示例：**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 35, 40],\n",
    "        'City': ['New York', 'London', 'Paris', 'Sydney']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 按 Age 列升序排序\n",
    "df_sorted_ascending = df.sort_values(by='Age', ascending=True)\n",
    "\n",
    "# 按 Age 列降序排序\n",
    "df_sorted_descending = df.sort_values(by='Age', ascending=False)\n",
    "\n",
    "# 按 Age 和 City 列升序排序\n",
    "df_sorted_multiple_columns = df.sort_values(by=['Age', 'City'], ascending=[True, True])\n",
    "\n",
    "# 就地修改原始DataFrame，按 Age 列降序排序\n",
    "df.sort_values(by='Age', ascending=False, inplace=True)\n",
    "```\n",
    "\n",
    "以上示例中，我们演示了`sort_values()`方法在DataFrame中的使用。通过指定`by`和`ascending`参数，您可以根据指定的列的值进行升序或降序排序。可以根据实际需求，选择合适的列和排序顺序来排序DataFrame或Series。\n",
    "\n",
    "\n",
    "- `sort_index()`: 按索引对DataFrame进行排序。\n",
    "\n",
    "5. 数据过滤和查询：\n",
    "\n",
    "- 使用条件表达式进行过滤。\n",
    "- 使用`isin()`方法来检查元素是否在某个列表中。\n",
    "\n",
    "6. 数据处理：\n",
    "\n",
    "- `apply()`: 对DataFrame的行或列应用自定义函数。\n",
    "- `map()`: 对Series的元素应用自定义函数。\n",
    "\n",
    "以上仅是DataFrame的部分常用方法，pandas库还有很多其他功能强大的函数和方法，可以根据实际需求灵活运用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78c7038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sites': [{'name': '菜鸟教程', 'url': 'www.runoob.com'}, {'name': 'google', 'url': 'www.google.com'}, {'name': '微博', 'url': 'www.weibo.com'}]}\n",
      "[{'name': '菜鸟教程', 'url': 'www.runoob.com'}, {'name': 'google', 'url': 'www.google.com'}, {'name': '微博', 'url': 'www.weibo.com'}]\n",
      "菜鸟教程, www.runoob.com\n",
      "google, www.google.com\n",
      "微博, www.weibo.com\n"
     ]
    }
   ],
   "source": [
    "data_str = '''{\n",
    "    \"sites\": [\n",
    "    { \"name\":\"菜鸟教程\" , \"url\":\"www.runoob.com\" }, \n",
    "    { \"name\":\"google\" , \"url\":\"www.google.com\" }, \n",
    "    { \"name\":\"微博\" , \"url\":\"www.weibo.com\" }\n",
    "    ]\n",
    "}'''\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "data_dict = json.loads(data_str)\n",
    "print(data_dict)\n",
    "\n",
    "\n",
    "sites_list = data_dict[\"sites\"]\n",
    "print(sites_list)\n",
    "for site in sites_list:\n",
    "    print(f'{site[\"name\"]}, {site[\"url\"]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c281119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==================================dict\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sites     3 non-null      object\n",
      " 1   number    3 non-null      int64 \n",
      " 2   user_num  3 non-null      int64 \n",
      " 3   country   3 non-null      object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 224.0+ bytes\n",
      "None\n",
      "#==================================csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 458 entries, 0 to 457\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Name      457 non-null    object \n",
      " 1   Team      457 non-null    object \n",
      " 2   Number    457 non-null    float64\n",
      " 3   Position  457 non-null    object \n",
      " 4   Age       457 non-null    float64\n",
      " 5   Height    457 non-null    object \n",
      " 6   Weight    457 non-null    float64\n",
      " 7   College   373 non-null    object \n",
      " 8   Salary    446 non-null    float64\n",
      "dtypes: float64(4), object(5)\n",
      "memory usage: 32.3+ KB\n",
      "None\n",
      "#==================================json\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3 non-null      object\n",
      " 1   name    3 non-null      object\n",
      " 2   url     3 non-null      object\n",
      " 3   likes   3 non-null      int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 224.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#==================================data types\n",
    "print(\"#==================================dict\")\n",
    "mydataset = {\n",
    "    'sites': [\"Google\", \"Runoob\", \"Wiki\"],\n",
    "    'number': [1,2,3],\n",
    "    'user_num': [10000000, 600000, 3000000],\n",
    "    'country': ['US', 'China', 'US']\n",
    "}\n",
    "\n",
    "mydf = pd.DataFrame(mydataset)\n",
    "print(mydf.info())\n",
    "\n",
    "\n",
    "#==================================csv\n",
    "print(\"#==================================csv\")\n",
    "csv_df = pd.read_csv('nba.csv')\n",
    "print(csv_df.info())\n",
    "\n",
    "#==================================json\n",
    "print(\"#==================================json\")\n",
    "json_df = pd.read_json('sites.json')\n",
    "print(json_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8aa5a",
   "metadata": {},
   "source": [
    "### 第一部分:数据读取\n",
    "\n",
    "- 从文件读取数据\n",
    "- 从数据库读取数据\n",
    "\n",
    "#### 1.文件数据源\n",
    "\n",
    "##### 使用python的文件操作方法读取文本文件\n",
    "\n",
    "例如学生成绩表数据如下：\n",
    "```txt\n",
    "学号,数学,英语,语文,python\n",
    "1,90,81,84,53\n",
    "2,90,73,89,69\n",
    "3,95,59,97,55\n",
    "4,74,69,54,67\n",
    "5,72,54,51,99\n",
    "6,67,99,66,81\n",
    "7,56,99,11\n",
    "```\n",
    "\n",
    "```python\n",
    "filename = 'grades.txt'\n",
    "\n",
    "student_grade = {}\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.startswith('学号'):\n",
    "            continue\n",
    "        data = line.strip().split(',')\n",
    "        if len(data) < 5:\n",
    "            continue\n",
    "        sid = int(data[0])\n",
    "        math = int(data[1])\n",
    "        english = int(data[2])\n",
    "        chinese = int(data[3])\n",
    "        python = int(data[4])\n",
    "        grades = [math, english, chinese, python]\n",
    "        total = sum(grades)\n",
    "        student_grade[sid] = [math, english, chinese, python]\n",
    "        \n",
    "```\n",
    "\n",
    "##### 使用Pandas读取结构化数据\n",
    "\n",
    "在Python中，使用pandas库可以方便地读取各种数据格式的文件，并将其转换为DataFrame对象，从而进行数据分析和处理。pandas支持读取多种数据源，包括CSV文件、Excel文件、数据库表、JSON文件、HTML表格等。\n",
    "\n",
    "以下是pandas读取数据的常用函数：\n",
    "\n",
    "1. **读取CSV文件**：使用`pandas.read_csv()`函数来读取CSV文件。\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件并将其转换为DataFrame对象\n",
    "df = pd.read_csv('student_info.csv')\n",
    "```\n",
    "\n",
    "其中`student_info.csv`是与上述程序位于同一目录的数据文件。内容如下：\n",
    "```txt\n",
    "StudentID,Name,Age,Gender,Grade\n",
    "1,Alice,20,Female,A\n",
    "2,Bob,21,Male,B\n",
    "3,Cathy,19,Female,B\n",
    "4,David,22,Male,A\n",
    "5,Eva,20,Female,A\n",
    "```\n",
    "\n",
    "2. **读取Excel文件**：使用`pandas.read_excel()`函数来读取Excel文件。\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 读取Excel文件并将其转换为DataFrame对象\n",
    "df = pd.read_excel('stock_prices.xlsx')\n",
    "```\n",
    "\n",
    "其中`stock_prices.xlsx`是与上述程序位于同一目录的数据文件。内容如下：\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "3. **读取数据库表**：使用`pandas.read_sql()`函数来读取数据库表。\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# 连接到数据库\n",
    "conn = sqlite3.connect('database.db')\n",
    "\n",
    "# 读取数据库表并将其转换为DataFrame对象\n",
    "query = 'SELECT * FROM table_name'\n",
    "df = pd.read_sql(query, conn)\n",
    "```\n",
    "\n",
    "4. 读取JSON文件：使用`pandas.read_json()`函数来读取JSON文件。\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 读取JSON文件并将其转换为DataFrame对象\n",
    "df = pd.read_json('data.json')\n",
    "```\n",
    "\n",
    "5. 读取HTML表格：使用`pandas.read_html()`函数来读取HTML表格。\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 读取HTML表格并将其转换为DataFrame对象\n",
    "tables = pd.read_html('data.html')\n",
    "df = tables[0]  # 假设第一个表格是我们需要的数据\n",
    "```\n",
    "\n",
    "注意：在使用这些函数之前，需要先安装pandas库，可以使用以下命令进行安装：\n",
    "```\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "\n",
    "读取数据后，我们就可以通过DataFrame对象进行数据分析、处理和操作了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e87f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   学号  数学  英语  语文  python\n",
      "0   1  90  81  84      53\n",
      "1   2  90  73  89      69\n",
      "2   3  95  59  97      55\n",
      "3   4  74  69  54      67\n",
      "4   5  72  54  51      99\n",
      "               学号          数学         英语          语文      python\n",
      "count  200.000000  200.000000  200.00000  200.000000  200.000000\n",
      "mean   100.500000   75.325000   75.83500   75.185000   73.930000\n",
      "std     57.879185   14.112234   14.65166   15.118652   14.890728\n",
      "min      1.000000   50.000000   50.00000   50.000000   50.000000\n",
      "25%     50.750000   63.750000   64.00000   62.000000   61.750000\n",
      "50%    100.500000   74.000000   75.00000   74.500000   73.500000\n",
      "75%    150.250000   88.000000   88.25000   89.000000   86.250000\n",
      "max    200.000000  100.000000  100.00000  100.000000   99.000000\n"
     ]
    }
   ],
   "source": [
    "#方案一：使用pandas读取数据\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('grades.txt')\n",
    "\n",
    "print(df.head())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfd572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#方案二：使用open函数读取文件\n",
    "\n",
    "# 读取学生成绩表\n",
    "filename = 'grades.txt'\n",
    "\n",
    "student_grade = {}\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.startswith('学号'):\n",
    "            continue\n",
    "        data = line.strip().split(',')\n",
    "        if len(data) < 5:\n",
    "            continue\n",
    "        sid = int(data[0])\n",
    "        math = int(data[1])\n",
    "        english = int(data[2])\n",
    "        chinese = int(data[3])\n",
    "        python = int(data[4])\n",
    "        grades = [math, english, chinese, python]\n",
    "        total = sum(grades)\n",
    "        average = total*1.0/len(grades)\n",
    "        grades.append(total)\n",
    "        grades.append(average)\n",
    "        \n",
    "        student_grade[sid] = grades\n",
    "print(len(student_grade))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacacced",
   "metadata": {},
   "source": [
    "### 第二部分：数据处理\n",
    "\n",
    "1. 数据解析：将原始数据解析为可操作的数据结构，如将CSV文件解析为数据表、将JSON数据解析为字典等。\n",
    "2. 数据转换：根据分析需求对数据进行转换和重塑，如重新排列数据列、进行数据类型转换、计算新的派生字段等。\n",
    "3. 数据合并：将多个数据源的数据合并为一个数据集，通过共享的键或索引将数据连接在一起。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9bbc647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   学号      200 non-null    int64\n",
      " 1   数学      200 non-null    int64\n",
      " 2   英语      200 non-null    int64\n",
      " 3   语文      200 non-null    int64\n",
      " 4   python  200 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 7.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5417d9",
   "metadata": {},
   "source": [
    "### 第三部分：数据清洗\n",
    "\n",
    "1. 缺失值处理：识别和处理数据中的缺失值，可以使用填充、插值等方法填补缺失值，或者根据数据的特性决定是否舍弃含有缺失值的观测。\n",
    "\n",
    "\n",
    "\n",
    "2. 异常值处理：检测和处理数据中的异常值，可以使用统计方法、数据分布、领域知识等进行异常值的识别和处理。\n",
    "\n",
    "\n",
    "3. 数据去重：识别和删除数据中的重复记录，确保数据集中的每个记录都是唯一的。\n",
    "\n",
    "\n",
    "4. 格式统一化：对数据进行格式的统一，如日期格式的统一、字符编码的转换等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07009c50",
   "metadata": {},
   "source": [
    "## 第四部分：Pandas系统学习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f379c15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b8798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b80a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sites     3 non-null      object\n",
      " 1   number    3 non-null      int64 \n",
      " 2   user_num  3 non-null      int64 \n",
      " 3   country   3 non-null      object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 224.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mydataset = {\n",
    "    'sites': [\"Google\", \"Runoob\", \"Wiki\"],\n",
    "    'number': [1,2,3],\n",
    "    'user_num': [10000000, 600000, 3000000],\n",
    "    'country': ['US', 'China', 'US']\n",
    "}\n",
    "\n",
    "mydf = pd.DataFrame(mydataset)\n",
    "print(mydf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88cac1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>number</th>\n",
       "      <th>user_num</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>10000000</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Runoob</td>\n",
       "      <td>2</td>\n",
       "      <td>600000</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>3</td>\n",
       "      <td>3000000</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sites  number  user_num country\n",
       "0  Google       1  10000000      US\n",
       "1  Runoob       2    600000   China\n",
       "2    Wiki       3   3000000      US"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260101aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>user_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.533333e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.883987e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.800000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.5</td>\n",
       "      <td>6.500000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       number      user_num\n",
       "count     3.0  3.000000e+00\n",
       "mean      2.0  4.533333e+06\n",
       "std       1.0  4.883987e+06\n",
       "min       1.0  6.000000e+05\n",
       "25%       1.5  1.800000e+06\n",
       "50%       2.0  3.000000e+06\n",
       "75%       2.5  6.500000e+06\n",
       "max       3.0  1.000000e+07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1942fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series = pd.Series([1,2,3,4,5], [0,1,2,3,4], name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5268d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(my_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9a8a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(my_series[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1861b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x    Google\n",
      "y    Runoob\n",
      "z      Wiki\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "a = [\"Google\", \"Runoob\", \"Wiki\"]\n",
    "\n",
    "myvar = pd.Series(a, index = [\"x\", \"y\", \"z\"])\n",
    "\n",
    "print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3207fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runoob\n"
     ]
    }
   ],
   "source": [
    "print(myvar['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c64e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xx    Google\n",
       "yy    Runoob\n",
       "zz      Wiki\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'xx': \"Google\", \"yy\": \"Runoob\", \"zz\": \"Wiki\"}\n",
    "mys = pd.Series(data)\n",
    "mys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38df0b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xx    Google\n",
       "zz      Wiki\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspart = pd.Series(data, index=['xx', 'zz'])\n",
    "myspart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fc55204",
   "metadata": {},
   "outputs": [],
   "source": [
    "myspart.name = 'test-series'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3db4c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 2 entries, xx to zz\n",
      "Series name: test-series\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "2 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 140.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "myspart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1358aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>number</th>\n",
       "      <th>user_num</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>10000000</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Runoob</td>\n",
       "      <td>2</td>\n",
       "      <td>600000</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>3</td>\n",
       "      <td>3000000</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sites  number  user_num country\n",
       "0  Google       1  10000000      US\n",
       "1  Runoob       2    600000   China\n",
       "2    Wiki       3   3000000      US"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51874384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n"
     ]
    }
   ],
   "source": [
    "print(mydf['sites'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79e36ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sites         Google\n",
      "number             1\n",
      "user_num    10000000\n",
      "country           US\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#第一行\n",
    "print(mydf.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d30494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sites  number  user_num country\n",
      "0  Google       1  10000000      US\n",
      "1  Runoob       2    600000   China\n"
     ]
    }
   ],
   "source": [
    "#第一行和第二行\n",
    "print(mydf.loc[[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432919ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    calories  duration\n",
      "d1       420        50\n",
      "d2       380        40\n",
      "d3       390        45\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"calories\": [420, 380, 390],\n",
    "    \"duration\": [50, 40, 45]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=['d1', 'd2', 'd3'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfcc808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calories    420\n",
      "duration     50\n",
      "Name: d1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.loc['d1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd44024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name            Team  Number Position   Age Height  Weight  \\\n",
      "0    Avery Bradley  Boston Celtics     0.0       PG  25.0    6-2   180.0   \n",
      "1      Jae Crowder  Boston Celtics    99.0       SF  25.0    6-6   235.0   \n",
      "2     John Holland  Boston Celtics    30.0       SG  27.0    6-5   205.0   \n",
      "3      R.J. Hunter  Boston Celtics    28.0       SG  22.0    6-5   185.0   \n",
      "4    Jonas Jerebko  Boston Celtics     8.0       PF  29.0   6-10   231.0   \n",
      "..             ...             ...     ...      ...   ...    ...     ...   \n",
      "453   Shelvin Mack       Utah Jazz     8.0       PG  26.0    6-3   203.0   \n",
      "454      Raul Neto       Utah Jazz    25.0       PG  24.0    6-1   179.0   \n",
      "455   Tibor Pleiss       Utah Jazz    21.0        C  26.0    7-3   256.0   \n",
      "456    Jeff Withey       Utah Jazz    24.0        C  26.0    7-0   231.0   \n",
      "457            NaN             NaN     NaN      NaN   NaN    NaN     NaN   \n",
      "\n",
      "               College     Salary  \n",
      "0                Texas  7730337.0  \n",
      "1            Marquette  6796117.0  \n",
      "2    Boston University        NaN  \n",
      "3        Georgia State  1148640.0  \n",
      "4                  NaN  5000000.0  \n",
      "..                 ...        ...  \n",
      "453             Butler  2433333.0  \n",
      "454                NaN   900000.0  \n",
      "455                NaN  2900000.0  \n",
      "456             Kansas   947276.0  \n",
      "457                NaN        NaN  \n",
      "\n",
      "[458 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('nba.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cecd29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name            Team  Number Position   Age Height  \\\n",
      "0             Avery Bradley  Boston Celtics     0.0       PG  25.0    6-2   \n",
      "1               Jae Crowder  Boston Celtics    99.0       SF  25.0    6-6   \n",
      "2              John Holland  Boston Celtics    30.0       SG  27.0    6-5   \n",
      "3               R.J. Hunter  Boston Celtics    28.0       SG  22.0    6-5   \n",
      "4             Jonas Jerebko  Boston Celtics     8.0       PF  29.0   6-10   \n",
      "5              Amir Johnson  Boston Celtics    90.0       PF  29.0    6-9   \n",
      "6             Jordan Mickey  Boston Celtics    55.0       PF  21.0    6-8   \n",
      "7              Kelly Olynyk  Boston Celtics    41.0        C  25.0    7-0   \n",
      "8              Terry Rozier  Boston Celtics    12.0       PG  22.0    6-2   \n",
      "9              Marcus Smart  Boston Celtics    36.0       PG  22.0    6-4   \n",
      "10          Jared Sullinger  Boston Celtics     7.0        C  24.0    6-9   \n",
      "11            Isaiah Thomas  Boston Celtics     4.0       PG  27.0    5-9   \n",
      "12              Evan Turner  Boston Celtics    11.0       SG  27.0    6-7   \n",
      "13              James Young  Boston Celtics    13.0       SG  20.0    6-6   \n",
      "14             Tyler Zeller  Boston Celtics    44.0        C  26.0    7-0   \n",
      "15         Bojan Bogdanovic   Brooklyn Nets    44.0       SG  27.0    6-8   \n",
      "16             Markel Brown   Brooklyn Nets    22.0       SG  24.0    6-3   \n",
      "17          Wayne Ellington   Brooklyn Nets    21.0       SG  28.0    6-4   \n",
      "18  Rondae Hollis-Jefferson   Brooklyn Nets    24.0       SG  21.0    6-7   \n",
      "19             Jarrett Jack   Brooklyn Nets     2.0       PG  32.0    6-3   \n",
      "\n",
      "    Weight            College      Salary  \n",
      "0    180.0              Texas   7730337.0  \n",
      "1    235.0          Marquette   6796117.0  \n",
      "2    205.0  Boston University         NaN  \n",
      "3    185.0      Georgia State   1148640.0  \n",
      "4    231.0                NaN   5000000.0  \n",
      "5    240.0                NaN  12000000.0  \n",
      "6    235.0                LSU   1170960.0  \n",
      "7    238.0            Gonzaga   2165160.0  \n",
      "8    190.0         Louisville   1824360.0  \n",
      "9    220.0     Oklahoma State   3431040.0  \n",
      "10   260.0         Ohio State   2569260.0  \n",
      "11   185.0         Washington   6912869.0  \n",
      "12   220.0         Ohio State   3425510.0  \n",
      "13   215.0           Kentucky   1749840.0  \n",
      "14   253.0     North Carolina   2616975.0  \n",
      "15   216.0                NaN   3425510.0  \n",
      "16   190.0     Oklahoma State    845059.0  \n",
      "17   200.0     North Carolina   1500000.0  \n",
      "18   220.0            Arizona   1335480.0  \n",
      "19   200.0       Georgia Tech   6300000.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24c6d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name       Team  Number Position   Age Height  Weight  \\\n",
      "448  Gordon Hayward  Utah Jazz    20.0       SF  26.0    6-8   226.0   \n",
      "449     Rodney Hood  Utah Jazz     5.0       SG  23.0    6-8   206.0   \n",
      "450      Joe Ingles  Utah Jazz     2.0       SF  28.0    6-8   226.0   \n",
      "451   Chris Johnson  Utah Jazz    23.0       SF  26.0    6-6   206.0   \n",
      "452      Trey Lyles  Utah Jazz    41.0       PF  20.0   6-10   234.0   \n",
      "453    Shelvin Mack  Utah Jazz     8.0       PG  26.0    6-3   203.0   \n",
      "454       Raul Neto  Utah Jazz    25.0       PG  24.0    6-1   179.0   \n",
      "455    Tibor Pleiss  Utah Jazz    21.0        C  26.0    7-3   256.0   \n",
      "456     Jeff Withey  Utah Jazz    24.0        C  26.0    7-0   231.0   \n",
      "457             NaN        NaN     NaN      NaN   NaN    NaN     NaN   \n",
      "\n",
      "      College      Salary  \n",
      "448    Butler  15409570.0  \n",
      "449      Duke   1348440.0  \n",
      "450       NaN   2050000.0  \n",
      "451    Dayton    981348.0  \n",
      "452  Kentucky   2239800.0  \n",
      "453    Butler   2433333.0  \n",
      "454       NaN    900000.0  \n",
      "455       NaN   2900000.0  \n",
      "456    Kansas    947276.0  \n",
      "457       NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0be7da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 458 entries, 0 to 457\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Name      457 non-null    object \n",
      " 1   Team      457 non-null    object \n",
      " 2   Number    457 non-null    float64\n",
      " 3   Position  457 non-null    object \n",
      " 4   Age       457 non-null    float64\n",
      " 5   Height    457 non-null    object \n",
      " 6   Weight    457 non-null    float64\n",
      " 7   College   373 non-null    object \n",
      " 8   Salary    446 non-null    float64\n",
      "dtypes: float64(4), object(5)\n",
      "memory usage: 32.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd5b0238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id    name             url  likes\n",
      "0  A001    菜鸟教程  www.runoob.com     61\n",
      "1  A002  Google  www.google.com    124\n",
      "2  A003      淘宝  www.taobao.com     45\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('sites.json')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caa68dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      col1 col2\n",
      "row1     1    x\n",
      "row2     2    y\n",
      "row3     3    z\n"
     ]
    }
   ],
   "source": [
    "s = {\n",
    "    \"col1\": {\n",
    "        \"row1\": 1,\n",
    "        \"row2\": 2,\n",
    "        \"row3\": 3\n",
    "    },\n",
    "    \"col2\": {\n",
    "        \"row1\": \"x\",\n",
    "        \"row2\": \"y\",\n",
    "        \"row3\": \"z\"\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(s)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d2759ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '''{\n",
    "    \"school_name\": \"local primary school\",\n",
    "    \"class\": \"Year 1\",\n",
    "    \"info\": {\n",
    "        \"president\": \"John Kasich\",\n",
    "        \"address\": \"ABC road, London, UK\",\n",
    "        \"contacts\": {\n",
    "        \"email\": \"admin@e.com\",\n",
    "        \"tel\": \"123456789\"\n",
    "        }\n",
    "    },\n",
    "    \"students\": [{\n",
    "\"id\": \"A001\",\n",
    "\"name\": \"Tom\",\n",
    "\"math\": 60,\n",
    "\"physics\": 66,\n",
    "\"chemistry\": 61\n",
    "},\n",
    "{\n",
    "\"id\": \"A002\",\n",
    "\"name\": \"James\",\n",
    "\"math\": 89,\n",
    "\"physics\": 76,\n",
    "\"chemistry\": 51\n",
    "},\n",
    "{\n",
    "\"id\": \"A003\",\n",
    "\"name\": \"Jenny\",\n",
    "\"math\": 79,\n",
    "\"physics\": 90,\n",
    "\"chemistry\": 78\n",
    "}]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdcd3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dic = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e84be933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(dic, record_path=['students'], meta = [\n",
    "    'class',\n",
    "    'school_name',\n",
    "    ['info', 'president'],\n",
    "    ['info', 'contacts', 'tel'],\n",
    "    ['info', 'contacts', 'email']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "932d8a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id   name  math  physics  chemistry   class           school_name  \\\n",
      "0  A001    Tom    60       66         61  Year 1  local primary school   \n",
      "1  A002  James    89       76         51  Year 1  local primary school   \n",
      "2  A003  Jenny    79       90         78  Year 1  local primary school   \n",
      "\n",
      "  info.president info.contacts.tel info.contacts.email  \n",
      "0    John Kasich         123456789         admin@e.com  \n",
      "1    John Kasich         123456789         admin@e.com  \n",
      "2    John Kasich         123456789         admin@e.com  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26ee51",
   "metadata": {},
   "source": [
    "## Pandas数据清洗\n",
    "\n",
    "Pandas是一个功能强大的Python库，用于数据处理和分析。在数据清洗过程中，Pandas提供了许多常用的方法和函数，可以帮助我们对数据进行预处理和整理。下面是一些常见的数据清洗操作：\n",
    "\n",
    "1. 去除重复值：使用`drop_duplicates()`方法可以去除DataFrame中的重复行。\n",
    "\n",
    "2. 处理缺失值：使用`fillna()`方法可以填充缺失值，使用`dropna()`方法可以去除包含缺失值的行。\n",
    "\n",
    "3. 数据类型转换：使用`astype()`方法可以将一列数据转换为指定的数据类型。\n",
    "\n",
    "4. 重命名列名：使用`rename()`方法可以对DataFrame的列名进行重命名。\n",
    "\n",
    "5. 数据排序：使用`sort_values()`方法可以对DataFrame按照指定的列进行排序。\n",
    "\n",
    "6. 数据切片：使用`loc[]`和`iloc[]`方法可以对DataFrame进行切片操作。\n",
    "\n",
    "7. 数据合并：使用`merge()`方法可以将多个DataFrame按照指定的键合并。\n",
    "\n",
    "8. 数据聚合：使用`groupby()`方法可以对DataFrame进行分组聚合操作。\n",
    "\n",
    "9. 数据去除异常值：使用条件过滤可以去除DataFrame中的异常值。\n",
    "\n",
    "10. 数据转换：使用`apply()`方法可以对DataFrame的每一列进行函数转换。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59c01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
